> 处理器体系结构
	> 指令集体系结构
		> 一个处理器支持的指令和指令的字节级编码称为它的指令集体系结构（Instruction-Set Architecture)
		> ISA在编译器编写者和处理器设计者之间提供了一个抽象概念层，作为软硬件接口，解决软硬件间的兼容性和可移植性问题，有了ISA才有通用计算机
		> (汇编 --> 机器码 的这种“翻译规则”) + (CPU执行机器码的逻辑电路) = 指令集
		> ISA定义了
			> 状态单元 Stat
			> 指令集和它们的编码
			> 一组编程规范
			> 异常事件处理
		> CISC 复杂指令集计算机(x86)
			> 指令系统复杂，指令数量多
			> 多为微程序控制
			> 有些指令延迟很长
			> 指令变长编码
			> 指定操作数的方式多样
			> 对指令的访存没有限制，多种指令都可访存
			> 对机器级程序透明
			> 有条件码
			> 编译器难以优化
			> 栈密集的过程链接（栈被用来存取过程参数和返回地址）
		> RISC 精简指令集计算机
			> 指令系统精简，保留少量常用指令
			> 多数为组合逻辑控制
			> 没有较长延迟的指令
			> 指令定长编码
			> 简单寻址方式，通常只有基址和偏移量寻址
			> 只有load/store访存
			> 对机器级程序来说细节可见
			> 无条件码
			> 易于编译优化
			> 寄存器密集的过程链接（因为对内存只提供简单的读写）
			> 通用寄存器多
			> RISC更好地支持流水线
		> RISC和amdhal的关系
			> RISC体现了加速大概率事件（common case fast）的思想，当我们试图提高指令集系统的整体性能时，高频指令对系统效率的影响更大
	> 时序电路（时序逻辑）
		> 有状态并且在这个状态上进行计算的系统
		> 必须引入按位存储信息的设备，这些设备由时钟控制
			> 时钟寄存器（寄存器），时钟信号控制寄存器加载输入值
			> 随机访问存储器（内存）
		> 对于一个寄存器，新的输入到来时，时钟低电平，则输出保持不变，时钟变为高电平，输出变化并保持到下一个上升沿
	> 组合电路（组合逻辑）
		> 组合逻辑不需要任何时序或控制，只要输入变化了，值就通过逻辑门网络传播
		> 组合电路当前的输出状态只取决于当前的输入，与之前的电路状态无关；时序电路的输出则与输入和电路原来的状态都有关
	> cpu指令处理阶段
		> 取指(fetch)
			> 从内存读取指令至指令寄存器(IR)，高4位为代码icode，低四位为功能ifun，地址为PC的值
			> 计算下一条PC的值存在valP中
		> 译码(decode)
			> 根据指令格式拆分指令，从寄存器文件读入至多两个（寄存器文件读写端口各两个）操作数。
		> 执行(execute)
			> 根据指令代码(icode)和指令功能(ifun的值)，执行操作
		> 访存(memory)
			> 从内存读/写数据
		> 写回(write back)
			> 将运行结果写回寄存器文件（如%rsp），方便后续指令使用
		> 更新PC(PC update),在指令流水线中并不算做一个单独的阶段
			> PC<-valP
	> 单周期处理器(SEQ)：在一个周期内完成上述操作的处理器
		> SEQ设计原则：从不回读。处理器从来不需要为了完成一条指令的执行而去读由该指令更新了的状态
		> SEQ不能充分利用硬件资源，因此速度很慢
	> 流水线处理器的基本原理
		> 概念
			> 流水线：将指令划分成若干阶段，每个阶段在其独立使用的硬件上与使用其他硬件的指令并行，提高了系统吞吐量
			> 延迟：从头到尾执行一条指令所需的时间，为吞吐量的倒数
			> 一个时钟周期：最长的逻辑组合的耗时+流水线寄存器开销
			> 流水线的代价是增加一些硬件（流水线寄存器，放在各个阶段之间，存储上一个阶段的结果），
			  以及延迟的少量增加，延迟变大是由于增加的流水线寄存器的时间开销。
		> 流水线的局限性
			> 不一致划分：各阶段的延迟不等，而时钟周期由延迟最大的阶段决定
			> 流水线过深：由流水线寄存器更新引起的延迟会降低收益
		> 怎么分段才能使加速比趋向于理想中的加速比？
			> 尽量使得划分出的每个阶段的延迟接近
			> 划分出的段数不能过多，因为要考虑到流水线寄存器产生的延迟
		> 流水线加速比
			> Tk=kt +(n-1)t =[k+(n-1)]t   n条指令，k段流水，t为一段的延迟
			> 加速比S=T0/Tk=nkt/Tk≈k
			> 所以加速比近似等于流水段数
	> 流水线冒险 Pipeline Hazard
		> 相关
			> 数据相关：下一条指令会用到这一条指令的计算结果
			> 控制相关：一条指令要确定下一条指令（jmp，call，ret）的位置
		> 冒险
			> 数据冒险：一条指令需要某数据而该数据正被之前的指令操作
			> 避免数据冒险的方法
				> 插入暂停阶段（插入nop指令）：让一组指令阻塞在他们所处的阶段，而允许其他指令继续通过流水线（执行阶段直接传到另一条的译码阶段）
				> 转发（旁路）：通过额外的数据通路将一条指令的结果值直接从一个阶段传到另一条指令较早阶段的技术
								（直接从执行阶段传而不是等到写回寄存器之后再由其他指令在译码阶段从寄存器读取）
				> 加载互锁：暂停与转发相结合以解决加载/使用数据冒险。因为加载是访存阶段完成，比执行阶段更靠后。（访存阶段直接传到另一条的译码阶段）
			> 控制冒险：处理器无法根据处于取指阶段的当前指令来确定下一条指令的地址（ret、jmp)
			> 避免控制冒险的方法
				> 插入暂停阶段（插入nop指令）
				> 分支预测：提前执行预测跳转指令选择的目标指令，如果预测错误，那么就取消（指令排除）那条提前执行的指令
				> 延迟转移：调整指令的顺序，将一定会执行的指令放在分支指令后面，这样流水线不停顿
			> 结构冒险（争用硬件）：因缺乏硬件支持而导致指令不能在预定的时钟周期内执行
			> 避免结构冒险
				> 插入暂停阶段（插入nop指令）
				> 设置相互独立的指令存储和数据存储（增加硬件）
	> 三种内部产生的异常
		> halt指令，导致处理器停止
		> 有非法指令和功能码组合的指令
		> 访问非法地址的指令
	
> 优化程序性能
	> Amdahl（是计算机系统设计的重要定量原理之一）（是一种性能分析定律）（答题的时候把思想和公式都写上）p16
		> Amdahl的主要思想：当我们对系统某部分加速时，该部分对系统整体性能的影响取决于该部分的重要性&加速程度
		> Amdahl的主要观点：想要显著加速整个系统，必须提升全系统中相当大部分的速度（common case fast），单靠一个方面是不行的
		> 公式：Tnew=[(1-a)+a/k]*T0（T0：执行某程序所需时间，a：系统某部分所需执行时间与T0的比例，k：该部分性能提升比例）
		> 加速比：S=T0/Tnew
	> 编写高效的程序
		> 在程序中利用局部性
		> 选择一组适当的算法和数据结构
		> 编写编译器能够有效优化的程序
		> 将一个任务分成多个部分，这些部分可以进行并行计算
	> 提高程序性能的方法（软件优化的方法）
		> 高级设计（算法优化）
			> 选择一组适当的算法和数据结构
		> 基本编码原则。避免限制优化的因素（代码优化）
			> 消除不必要的内存引用，引入临时变量来保存中间值。(减少访存）
			> 消除连续的函数调用，将要循环多次但计算结果不会改变的计算移到循环外（如i<str.size()）
			> 减少过程调用
		> 低级优化。结构化代码以利用硬件功能（指令优化）
			> 循环展开：通过增加每次迭代计算的元素数量，减少循环的迭代次数，减少了循环开销操作
			> 提高并行
				> 使用多个累积变量（分治思想）：将一组可结合和可交换的合并运算分割成多个部分，在最后合并结果以提高性能
				> 重新结合变换（加括号）：减少计算中关键路径上的操作数量，更好地利用功能单元的流水线能力
			> 书写适合用条件传送而非条件控制实现的代码
	> 优化编译器的能力和局限性
		> GCC能完成基本的优化，不会对程序进行“更有进取心的”编译器所做的激进变换。
		> 编译器只进行安全的优化，如内存别名使用，编译器必须假设不同的指针可能会指向内存中同一位置，这就限制了优化策略
		> 函数调用也会妨碍优化，编译器会假设最糟的情况（所有调用都有副作用（改变全局变量）），保持所有的函数调用不变
		> GCC用内联函数替换优化函数调用，但只尝试在单个文件中定义的函数的内联。
	> 表示程序性能	
		> 每元素的周期数（Cycles Per Element)，即线性因子。这对执行重复计算的程序来说是很适当的
		> 用“每个元素”而不是“每次循环”的周期数作为度量标准，是因为像循环展开这样的技术使得我们能用较少的循环次数完成计算
	> 特定体系结构或应用特性的性能优化
		> 超标量流水线
		> 乱序处理（延迟转移）
		> 分支预测+投机转移
	> 制约程序实际性能的因素
		> 寄存器溢出：并行度超出了可用的寄存器数量，这时会“溢出”，将某些临时值存放到内存中，通常是在运行时堆栈上分配空间
		> 分支预测错误
			> 解决方法
				> 不过分关心可预测的分支
				> 书写适合用条件传送而非条件控制实现的代码
	> 确认和消除性能瓶颈
		> 程序剖析：程序剖析运行程序的一个版本，其中插入了工具代码，以确定程序各个部分所需时间。
		> unix剖析程序GPROF
			> 确定每个函数花费了多少CPU时间
			> 计算每个函数被调用的次数，根据执行调用的函数来分类。
			> GPROF的一些属性
				> 计时不准确
				> 假设没有执行内联替换，则调用信息相当可靠
				> 默认情况下，不会显示对库函数的计时

> 存储器结构及虚拟存储器
	> 局部性原理
		> 时间局部性：当前被访问的内存位置可能很快再次被访问
			> 产生原因：循环操作
			> 实现方法：缓存机制
		> 空间局部性：当前被访问的内容附近的内容很快会被访问
			> 产生原因：指令或数据通常是连续存放的
			> 实现方法：预取机制，以页或簇为单位一次读取多条连续指令或数据
		> 编写良好的程序倾向于展现出良好的局部性
	
	> 存储器层次结构
		> 存储技术：不同存储技术的访问时间差异很大，速度较快的技术每字节的成本要比速度较慢的技术高，而且容量较小。CPU与主存之间的速度差距在增大
		> 层次结构：从高层往底层走，速度变慢，容量变大，价格降低
			> L0：寄存器
			> L1：高速缓存SRAM
			> L2：高速缓存SRAM
			> L3: 高速缓存SRAM
			> L4：主存DRAM
			> L5：本地二级存储（外存）ROM
			> L6: 远程二级存储
		> 中心思想：高一层作为低一层的缓存，层次结构中每一层都缓存来自较低一层的数据对象
		> 闪存用EPROM，读快写慢，因为写要先擦除再写
	> 计算机高速缓存器原理（内存映射晚点看）
		> Cache位于cpu和主存之间，是一个速度比主存更快，容量也更小的存储器，用于协调cpu与主存速度不匹配的问题
		> 当cpu向内存写/读数据时，这些数据就会存入cache，下次cpu再需要的时候就会优先到cache中读取
		> 广义上的高速缓存技术包括：快表&cache&虚拟内存技术
		
	> 高速缓存参数的性能影响
		> 高速缓存大小的影响：较大的高速缓存可能会提高命中率，但会增加命中时间（传送一个字到cpu的时间，包括租选择、行确认和字选择的时间）
		> 块大小的影响：较大的块有利于空间局部性，帮助提高命中率，但在cache大小固定的情况下，cache行数量会减少，有损时间局部性，同时不命中处罚也会增加。
		> 相联度的影响：较高的相联度降低了抖动发生的可能性，但会增加额外的控制逻辑，增加命中时间和不命中处罚。
		> 写策略的影响：写回法减少了与低一层的低速设备的数据传送。
	
	> 高速缓存对性能的影响
		> 利用时间局部性，使得频繁使用的字从L1取出，还要利用空间局部性，使得尽可能多地字从一个L1高速缓存行中访问到。
		
	> 地址空间
		> 私有地址空间：进程为每个程序提供一个假象，好像它独占地使用系统地址空间
		> 物理地址空间：对应于系统中一段连续的物理内存地址
		> 虚拟地址空间：对应于系统中一段连续的虚拟内存地址，即私有地址空间
		
	> 虚拟存储器
		> 虚拟内存是对主存的一个抽象，实际上是不存在的，只是提供了一种假象
		> 虚拟内存技术实际上就是建立了“内存-外存”两级存储结构，将内存作为缓存，并由此利用局部性原理
		> 虚拟存储器大小由计算机地址结构（寻址空间）决定
		> 作用：
			> 将内存作为“内存-外存”两级存储结构的缓存，由此利用局部性原理
			> 作为内存管理工具
				> 简化链接：独立的地址空间允许每个进程的内存映像使用相同的基本格式
				> 简化加载：在每个页初次被引用时，虚拟内存系统会按照需要自动地调入数据页
				> 简化共享：将不同进程中适当的虚拟页面映射到相同的物理页面
				> 简化内存分配：用户进程中要求堆空间，操作系统将k个连续的虚拟页面映射到任意k个物理页面中，而无需分配k个连续的物理页面
			> 作为内存保护的工具
				> 提供独立的地址空间易于区分不同进程的私有内存。
			
	> 虚拟内存的管理
		> 页表：将虚拟页映射到物理页。操作系统为每个进程提供了一个独立的页表，因而也就是一个独立的虚拟地址空间，页表存放在物理内存中。
			> 有效位
			> 物理页起始地址
			
	> 翻译和映射
		> 地址翻译：将虚拟地址变换为物理地址的过程
			> 处理器生成虚拟地址（虚拟页号+虚拟页偏移量），传送给MMU（memory management unit）
			> MMU生成PTE（page table entry（条目），页表项）地址，并从cache/内存请求得到它
			> cache/内存向MMU返回PTE（页表基址+虚拟页号）
			> MMU构造物理地址（物理页号+虚拟页偏移量），传送给cache/内存
			> cache/内存返回所请求的数据字给处理器
		> 内存映射
			> 概念：用与虚拟内存区域相关联的一个物理内存上的对象初始化这个虚拟内存区域的过程
			
	> TLB
		> 用以提高虚拟内存的性能
		> chache中也可存储PTE，因此TLB是页表的缓存，位于MMU中
		> 这样一来TLB命中时所有的地址翻译都在cpu的MMU中执行，避免了多次从内存中取PTE，因此非常快
		
	> 动态存储器分配
		> 使用动态内存分配原因：直到程序运行时，才知道某些数据结构的大小。
		> 动态内存分配器维护着一个进程的虚拟内存区域，称为堆。
		> 对于每个进程，内核维护一个brk指向堆顶
		> 显式分配器：要求显式地释放任何已分配的块（C和C++）
		> 隐式分配器：分配器自动检测和释放一个不再使用的块，因此隐式分配器也叫作垃圾收集器
		
	> 垃圾收集
		> 垃圾收集器是一种动态内存分配器，它自动释放程序不再需要的已分配块
		> 垃圾收集器将内存视为一张有向可达图，并通过释放不可达节点将它们返回给空闲链表，来定期回收它们。